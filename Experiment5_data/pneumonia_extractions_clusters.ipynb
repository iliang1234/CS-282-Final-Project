{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy.lib.stride_tricks import sliding_window_view\n",
        "import os, pickle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "from itertools import product\n",
        "\n",
        "import sys, os\n",
        "\n",
        "import trajectory as T                      # trajectory generation\n",
        "import optimizer as O                       # stochastic gradient descent optimizer\n",
        "import solver as S                          # MDP solver (value-iteration)\n",
        "import plot as P\n",
        "\n"
      ],
      "metadata": {
        "id": "cBmuqzqHfqT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOADING THE DATA"
      ],
      "metadata": {
        "id": "hxJYvR8zr1MV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print(len(data_df))\n",
        "# print(len(X_df))\n",
        "# print(len(y_df))\n",
        "# print(len(data_non_normalized_df))\n",
        "\n",
        "data_df = pd.read_pickle('data_df_pneumonia.pkl')\n",
        "X_df = pd.read_pickle('X_df_pneumonia.pkl')\n",
        "y_df = pd.read_pickle('y_df_pneumonia.pkl')\n",
        "data_non_normalized_df = pd.read_pickle('data_non_normalized_df_pneumonia.pkl')\n",
        "# note: data should have cluster assignments already"
      ],
      "metadata": {
        "id": "zSoMb1qcfsQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_non_normalized_df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQ27zBw7PZ3C",
        "outputId": "eb949ed2-1dd4-4360-c50e-94e36e44efcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['subject_id', 'Creatinine (serum)', 'Inspired O2 Fraction',\n",
              "       'Urine output_ApacheIV', 'ALT', 'AST', 'Arterial Blood Pressure mean',\n",
              "       'Arterial Blood Pressure diastolic', 'Arterial Blood Pressure systolic',\n",
              "       'GcsScore_ApacheIV', 'Arterial O2 pressure', 'Heart Rate',\n",
              "       'Temperature Celsius', 'Respiratory Rate',\n",
              "       'O2 saturation pulseoxymetry', 'action', 'cluster'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PARAMS:\n",
        "smoothing_value = 1\n",
        "discount_exp = 0.001\n",
        "action_bound = 15\n"
      ],
      "metadata": {
        "id": "_lP6vDxHRFVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Corresponding output data for training BC (corresponding treatments for each data point in X_df)"
      ],
      "metadata": {
        "id": "ty_UG34Um94a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gjw_qsM9m08j",
        "outputId": "5e76894d-c696-4f98-d01b-86cce5c4aeed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    2.0\n",
              "1    8.0\n",
              "2    8.0\n",
              "3    4.0\n",
              "4    0.0\n",
              "Name: action, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "mACowHHFP3jQ",
        "outputId": "9b932a43-21a6-4897-c839-87227d15f919"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                              subject_id  Creatinine (serum)  \\\n",
              "stay_id  time_hour                                             \n",
              "30002521 2162-05-03 10:00:00    13269859           -0.662381   \n",
              "         2164-02-07 10:00:00    13269859           -0.662381   \n",
              "         2164-02-08 08:00:00    13269859           -0.662381   \n",
              "30011071 2176-08-18 03:00:00    11885477           -0.662381   \n",
              "30028516 2179-06-28 18:00:00    15517908           -0.662381   \n",
              "\n",
              "                              Inspired O2 Fraction  Urine output_ApacheIV  \\\n",
              "stay_id  time_hour                                                          \n",
              "30002521 2162-05-03 10:00:00             -2.531506                    0.0   \n",
              "         2164-02-07 10:00:00             -2.531506                    0.0   \n",
              "         2164-02-08 08:00:00             -2.531506                    0.0   \n",
              "30011071 2176-08-18 03:00:00             -2.531506                    0.0   \n",
              "30028516 2179-06-28 18:00:00             -2.531506                    0.0   \n",
              "\n",
              "                                   ALT      AST  Arterial Blood Pressure mean  \\\n",
              "stay_id  time_hour                                                              \n",
              "30002521 2162-05-03 10:00:00 -0.142368 -0.14189                     -0.064513   \n",
              "         2164-02-07 10:00:00 -0.142368 -0.14189                     -0.064513   \n",
              "         2164-02-08 08:00:00 -0.142368 -0.14189                     -0.064513   \n",
              "30011071 2176-08-18 03:00:00 -0.142368 -0.14189                     -0.064513   \n",
              "30028516 2179-06-28 18:00:00 -0.142368 -0.14189                     -0.064513   \n",
              "\n",
              "                              Arterial Blood Pressure diastolic  \\\n",
              "stay_id  time_hour                                                \n",
              "30002521 2162-05-03 10:00:00                           0.136648   \n",
              "         2164-02-07 10:00:00                           0.136648   \n",
              "         2164-02-08 08:00:00                           0.136648   \n",
              "30011071 2176-08-18 03:00:00                           0.136648   \n",
              "30028516 2179-06-28 18:00:00                           0.136648   \n",
              "\n",
              "                              Arterial Blood Pressure systolic  \\\n",
              "stay_id  time_hour                                               \n",
              "30002521 2162-05-03 10:00:00                          0.035025   \n",
              "         2164-02-07 10:00:00                          0.035025   \n",
              "         2164-02-08 08:00:00                          0.035025   \n",
              "30011071 2176-08-18 03:00:00                          0.035025   \n",
              "30028516 2179-06-28 18:00:00                          0.035025   \n",
              "\n",
              "                              GcsScore_ApacheIV  Arterial O2 pressure  \\\n",
              "stay_id  time_hour                                                      \n",
              "30002521 2162-05-03 10:00:00                0.0              2.266593   \n",
              "         2164-02-07 10:00:00                0.0              2.266593   \n",
              "         2164-02-08 08:00:00                0.0              2.266593   \n",
              "30011071 2176-08-18 03:00:00                0.0              2.266593   \n",
              "30028516 2179-06-28 18:00:00                0.0              2.266593   \n",
              "\n",
              "                              Heart Rate  Temperature Celsius  \\\n",
              "stay_id  time_hour                                              \n",
              "30002521 2162-05-03 10:00:00    0.067084            -0.174096   \n",
              "         2164-02-07 10:00:00    0.661191            -0.174096   \n",
              "         2164-02-08 08:00:00    0.013074            -0.174096   \n",
              "30011071 2176-08-18 03:00:00    0.337132            -0.174096   \n",
              "30028516 2179-06-28 18:00:00    0.013074            -0.174096   \n",
              "\n",
              "                              Respiratory Rate  O2 saturation pulseoxymetry  \\\n",
              "stay_id  time_hour                                                            \n",
              "30002521 2162-05-03 10:00:00         -0.050875                     0.634716   \n",
              "         2164-02-07 10:00:00          0.292859                     0.076779   \n",
              "         2164-02-08 08:00:00         -0.910210                     0.634716   \n",
              "30011071 2176-08-18 03:00:00          0.636593                     1.192653   \n",
              "30028516 2179-06-28 18:00:00          0.464726                     1.192653   \n",
              "\n",
              "                              action  cluster  \n",
              "stay_id  time_hour                             \n",
              "30002521 2162-05-03 10:00:00     2.0       64  \n",
              "         2164-02-07 10:00:00     8.0       64  \n",
              "         2164-02-08 08:00:00     8.0       64  \n",
              "30011071 2176-08-18 03:00:00     4.0       58  \n",
              "30028516 2179-06-28 18:00:00     0.0        0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0cfefe4a-3d7e-4896-a385-a564852bc24e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>subject_id</th>\n",
              "      <th>Creatinine (serum)</th>\n",
              "      <th>Inspired O2 Fraction</th>\n",
              "      <th>Urine output_ApacheIV</th>\n",
              "      <th>ALT</th>\n",
              "      <th>AST</th>\n",
              "      <th>Arterial Blood Pressure mean</th>\n",
              "      <th>Arterial Blood Pressure diastolic</th>\n",
              "      <th>Arterial Blood Pressure systolic</th>\n",
              "      <th>GcsScore_ApacheIV</th>\n",
              "      <th>Arterial O2 pressure</th>\n",
              "      <th>Heart Rate</th>\n",
              "      <th>Temperature Celsius</th>\n",
              "      <th>Respiratory Rate</th>\n",
              "      <th>O2 saturation pulseoxymetry</th>\n",
              "      <th>action</th>\n",
              "      <th>cluster</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>stay_id</th>\n",
              "      <th>time_hour</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"3\" valign=\"top\">30002521</th>\n",
              "      <th>2162-05-03 10:00:00</th>\n",
              "      <td>13269859</td>\n",
              "      <td>-0.662381</td>\n",
              "      <td>-2.531506</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.142368</td>\n",
              "      <td>-0.14189</td>\n",
              "      <td>-0.064513</td>\n",
              "      <td>0.136648</td>\n",
              "      <td>0.035025</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.266593</td>\n",
              "      <td>0.067084</td>\n",
              "      <td>-0.174096</td>\n",
              "      <td>-0.050875</td>\n",
              "      <td>0.634716</td>\n",
              "      <td>2.0</td>\n",
              "      <td>64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2164-02-07 10:00:00</th>\n",
              "      <td>13269859</td>\n",
              "      <td>-0.662381</td>\n",
              "      <td>-2.531506</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.142368</td>\n",
              "      <td>-0.14189</td>\n",
              "      <td>-0.064513</td>\n",
              "      <td>0.136648</td>\n",
              "      <td>0.035025</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.266593</td>\n",
              "      <td>0.661191</td>\n",
              "      <td>-0.174096</td>\n",
              "      <td>0.292859</td>\n",
              "      <td>0.076779</td>\n",
              "      <td>8.0</td>\n",
              "      <td>64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2164-02-08 08:00:00</th>\n",
              "      <td>13269859</td>\n",
              "      <td>-0.662381</td>\n",
              "      <td>-2.531506</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.142368</td>\n",
              "      <td>-0.14189</td>\n",
              "      <td>-0.064513</td>\n",
              "      <td>0.136648</td>\n",
              "      <td>0.035025</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.266593</td>\n",
              "      <td>0.013074</td>\n",
              "      <td>-0.174096</td>\n",
              "      <td>-0.910210</td>\n",
              "      <td>0.634716</td>\n",
              "      <td>8.0</td>\n",
              "      <td>64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30011071</th>\n",
              "      <th>2176-08-18 03:00:00</th>\n",
              "      <td>11885477</td>\n",
              "      <td>-0.662381</td>\n",
              "      <td>-2.531506</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.142368</td>\n",
              "      <td>-0.14189</td>\n",
              "      <td>-0.064513</td>\n",
              "      <td>0.136648</td>\n",
              "      <td>0.035025</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.266593</td>\n",
              "      <td>0.337132</td>\n",
              "      <td>-0.174096</td>\n",
              "      <td>0.636593</td>\n",
              "      <td>1.192653</td>\n",
              "      <td>4.0</td>\n",
              "      <td>58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30028516</th>\n",
              "      <th>2179-06-28 18:00:00</th>\n",
              "      <td>15517908</td>\n",
              "      <td>-0.662381</td>\n",
              "      <td>-2.531506</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.142368</td>\n",
              "      <td>-0.14189</td>\n",
              "      <td>-0.064513</td>\n",
              "      <td>0.136648</td>\n",
              "      <td>0.035025</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.266593</td>\n",
              "      <td>0.013074</td>\n",
              "      <td>-0.174096</td>\n",
              "      <td>0.464726</td>\n",
              "      <td>1.192653</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0cfefe4a-3d7e-4896-a385-a564852bc24e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0cfefe4a-3d7e-4896-a385-a564852bc24e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0cfefe4a-3d7e-4896-a385-a564852bc24e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4fee2cb5-6852-420c-a0ce-7911fd5994e5\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4fee2cb5-6852-420c-a0ce-7911fd5994e5')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4fee2cb5-6852-420c-a0ce-7911fd5994e5 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_non_normalized_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "plrY_XzJP7eb",
        "outputId": "26e032b9-e744-4751-b2d2-9bd6092fe6cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                              subject_id  Creatinine (serum)  \\\n",
              "stay_id  time_hour                                             \n",
              "30002521 2162-05-03 10:00:00    13269859                 1.0   \n",
              "         2164-02-07 10:00:00    13269859                 1.0   \n",
              "         2164-02-08 08:00:00    13269859                 1.0   \n",
              "30011071 2176-08-18 03:00:00    11885477                 1.0   \n",
              "30028516 2179-06-28 18:00:00    15517908                 1.0   \n",
              "\n",
              "                              Inspired O2 Fraction  Urine output_ApacheIV  \\\n",
              "stay_id  time_hour                                                          \n",
              "30002521 2162-05-03 10:00:00                  0.21                   80.0   \n",
              "         2164-02-07 10:00:00                  0.21                   80.0   \n",
              "         2164-02-08 08:00:00                  0.21                   80.0   \n",
              "30011071 2176-08-18 03:00:00                  0.21                   80.0   \n",
              "30028516 2179-06-28 18:00:00                  0.21                   80.0   \n",
              "\n",
              "                               ALT   AST  Arterial Blood Pressure mean  \\\n",
              "stay_id  time_hour                                                       \n",
              "30002521 2162-05-03 10:00:00  34.0  40.0                          77.0   \n",
              "         2164-02-07 10:00:00  34.0  40.0                          77.0   \n",
              "         2164-02-08 08:00:00  34.0  40.0                          77.0   \n",
              "30011071 2176-08-18 03:00:00  34.0  40.0                          77.0   \n",
              "30028516 2179-06-28 18:00:00  34.0  40.0                          77.0   \n",
              "\n",
              "                              Arterial Blood Pressure diastolic  \\\n",
              "stay_id  time_hour                                                \n",
              "30002521 2162-05-03 10:00:00                               59.0   \n",
              "         2164-02-07 10:00:00                               59.0   \n",
              "         2164-02-08 08:00:00                               59.0   \n",
              "30011071 2176-08-18 03:00:00                               59.0   \n",
              "30028516 2179-06-28 18:00:00                               59.0   \n",
              "\n",
              "                              Arterial Blood Pressure systolic  \\\n",
              "stay_id  time_hour                                               \n",
              "30002521 2162-05-03 10:00:00                             118.0   \n",
              "         2164-02-07 10:00:00                             118.0   \n",
              "         2164-02-08 08:00:00                             118.0   \n",
              "30011071 2176-08-18 03:00:00                             118.0   \n",
              "30028516 2179-06-28 18:00:00                             118.0   \n",
              "\n",
              "                              GcsScore_ApacheIV  Arterial O2 pressure  \\\n",
              "stay_id  time_hour                                                      \n",
              "30002521 2162-05-03 10:00:00               11.0                 276.0   \n",
              "         2164-02-07 10:00:00               11.0                 276.0   \n",
              "         2164-02-08 08:00:00               11.0                 276.0   \n",
              "30011071 2176-08-18 03:00:00               11.0                 276.0   \n",
              "30028516 2179-06-28 18:00:00               11.0                 276.0   \n",
              "\n",
              "                              Heart Rate  Temperature Celsius  \\\n",
              "stay_id  time_hour                                              \n",
              "30002521 2162-05-03 10:00:00        86.0                 37.0   \n",
              "         2164-02-07 10:00:00        97.0                 37.0   \n",
              "         2164-02-08 08:00:00        85.0                 37.0   \n",
              "30011071 2176-08-18 03:00:00        91.0                 37.0   \n",
              "30028516 2179-06-28 18:00:00        85.0                 37.0   \n",
              "\n",
              "                              Respiratory Rate  O2 saturation pulseoxymetry  \\\n",
              "stay_id  time_hour                                                            \n",
              "30002521 2162-05-03 10:00:00              19.0                         98.0   \n",
              "         2164-02-07 10:00:00              21.0                         96.0   \n",
              "         2164-02-08 08:00:00              14.0                         98.0   \n",
              "30011071 2176-08-18 03:00:00              23.0                        100.0   \n",
              "30028516 2179-06-28 18:00:00              22.0                        100.0   \n",
              "\n",
              "                              action  cluster  \n",
              "stay_id  time_hour                             \n",
              "30002521 2162-05-03 10:00:00     2.0       64  \n",
              "         2164-02-07 10:00:00     8.0       64  \n",
              "         2164-02-08 08:00:00     8.0       64  \n",
              "30011071 2176-08-18 03:00:00     4.0       58  \n",
              "30028516 2179-06-28 18:00:00     0.0        0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9108b973-407c-4ee0-b944-a734d3c1eb00\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>subject_id</th>\n",
              "      <th>Creatinine (serum)</th>\n",
              "      <th>Inspired O2 Fraction</th>\n",
              "      <th>Urine output_ApacheIV</th>\n",
              "      <th>ALT</th>\n",
              "      <th>AST</th>\n",
              "      <th>Arterial Blood Pressure mean</th>\n",
              "      <th>Arterial Blood Pressure diastolic</th>\n",
              "      <th>Arterial Blood Pressure systolic</th>\n",
              "      <th>GcsScore_ApacheIV</th>\n",
              "      <th>Arterial O2 pressure</th>\n",
              "      <th>Heart Rate</th>\n",
              "      <th>Temperature Celsius</th>\n",
              "      <th>Respiratory Rate</th>\n",
              "      <th>O2 saturation pulseoxymetry</th>\n",
              "      <th>action</th>\n",
              "      <th>cluster</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>stay_id</th>\n",
              "      <th>time_hour</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"3\" valign=\"top\">30002521</th>\n",
              "      <th>2162-05-03 10:00:00</th>\n",
              "      <td>13269859</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.21</td>\n",
              "      <td>80.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>77.0</td>\n",
              "      <td>59.0</td>\n",
              "      <td>118.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>276.0</td>\n",
              "      <td>86.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2164-02-07 10:00:00</th>\n",
              "      <td>13269859</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.21</td>\n",
              "      <td>80.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>77.0</td>\n",
              "      <td>59.0</td>\n",
              "      <td>118.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>276.0</td>\n",
              "      <td>97.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>96.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2164-02-08 08:00:00</th>\n",
              "      <td>13269859</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.21</td>\n",
              "      <td>80.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>77.0</td>\n",
              "      <td>59.0</td>\n",
              "      <td>118.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>276.0</td>\n",
              "      <td>85.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30011071</th>\n",
              "      <th>2176-08-18 03:00:00</th>\n",
              "      <td>11885477</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.21</td>\n",
              "      <td>80.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>77.0</td>\n",
              "      <td>59.0</td>\n",
              "      <td>118.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>276.0</td>\n",
              "      <td>91.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30028516</th>\n",
              "      <th>2179-06-28 18:00:00</th>\n",
              "      <td>15517908</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.21</td>\n",
              "      <td>80.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>77.0</td>\n",
              "      <td>59.0</td>\n",
              "      <td>118.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>276.0</td>\n",
              "      <td>85.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9108b973-407c-4ee0-b944-a734d3c1eb00')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9108b973-407c-4ee0-b944-a734d3c1eb00 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9108b973-407c-4ee0-b944-a734d3c1eb00');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-bd0ac252-90a1-4989-8b09-a83aba675436\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bd0ac252-90a1-4989-8b09-a83aba675436')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-bd0ac252-90a1-4989-8b09-a83aba675436 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CLUSTER_LIST = data_non_normalized_df['cluster']\n"
      ],
      "metadata": {
        "id": "eKgir9uuPmE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Looking at the values counts for each cluster\n",
        "\n",
        "np.unique(CLUSTER_LIST, return_counts = True)[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2M06PxkEyHyE",
        "outputId": "765b7a6a-5746-4777-efbd-66ba69988ef3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 649, 7815,  161, 2438,  603,  435, 1259, 2400, 1263,   28, 1579,\n",
              "        254, 1238,  434,  303,  657,  725,  500,  655,  493,  530,  505,\n",
              "       2028,   96,  925,  290, 2168,  135,  937, 1859,  987,  110,  393,\n",
              "        599,  238,  147,  274,  165,  225,  124,  397,   21,  200,   73,\n",
              "        638,  300,  831,  643,  117,  500,  278,  196,  509,  251,  146,\n",
              "         64,  266,  822,  169,  539,  135,  399,  424,  248,  378,  557,\n",
              "         94,  148,  179,   81,  189,  343,  110,  248,   72,  133,  468,\n",
              "        144,  556,  594,  120,  403,  219,  614,  107,  315,   44,   56,\n",
              "        119,   42,  112,  392,  156,  284,   81,  142,   87,  289,  180,\n",
              "         63])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Converting the data into trajectories to input to an IRL algorithm Note this is the same format of trajectories we used for HW1 and HW2."
      ],
      "metadata": {
        "id": "Rhol7shFy4uA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_stay_ids = data_df.index.get_level_values('stay_id').unique()\n",
        "unique_subject_ids = set(data_df['subject_id'])\n",
        "trajectories = []\n",
        "\n",
        "for person in unique_subject_ids:\n",
        "  filtered_df = data_df[data_df['subject_id']==person].drop(columns=['subject_id'])\n",
        "  unique_stays = filtered_df.index.get_level_values('stay_id').unique()\n",
        "  for stay_id in unique_stays:\n",
        "    states, actions = filtered_df.loc[stay_id]['cluster'], filtered_df.loc[stay_id]['action']\n",
        "    # print(len(states))\n",
        "\n",
        "    trajectory = []\n",
        "    if len(states)<2:\n",
        "      continue\n",
        "    else:\n",
        "      for i in range(len(states) - 1):\n",
        "        trajectory.append((states[i], int(actions[i]), states[i+1] ))\n",
        "\n",
        "      trajectories.append(T.Trajectory(trajectory))"
      ],
      "metadata": {
        "id": "_orzJHTGmyur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to store all possible terminal states from the trajectories list. (Needed to calculate the normalizing constant in MaxEnt)"
      ],
      "metadata": {
        "id": "cSrWgKqX1eLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "terminal_states = []\n",
        "\n",
        "for traj in trajectories:\n",
        "  terminal_states.append(traj._t[-1][-1])\n",
        "\n",
        "terminal_states = list(set(terminal_states))"
      ],
      "metadata": {
        "id": "9CsdWcAlEiYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "terminal_states"
      ],
      "metadata": {
        "id": "3TN_1XNJEiV2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07bd8e26-d6fb-48fa-9432-952aad492f9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 4,\n",
              " 5,\n",
              " 6,\n",
              " 7,\n",
              " 8,\n",
              " 9,\n",
              " 10,\n",
              " 11,\n",
              " 12,\n",
              " 13,\n",
              " 14,\n",
              " 15,\n",
              " 16,\n",
              " 17,\n",
              " 18,\n",
              " 19,\n",
              " 20,\n",
              " 21,\n",
              " 22,\n",
              " 23,\n",
              " 24,\n",
              " 25,\n",
              " 26,\n",
              " 27,\n",
              " 28,\n",
              " 29,\n",
              " 30,\n",
              " 31,\n",
              " 32,\n",
              " 33,\n",
              " 34,\n",
              " 35,\n",
              " 36,\n",
              " 37,\n",
              " 38,\n",
              " 39,\n",
              " 40,\n",
              " 41,\n",
              " 42,\n",
              " 43,\n",
              " 44,\n",
              " 45,\n",
              " 46,\n",
              " 47,\n",
              " 48,\n",
              " 49,\n",
              " 50,\n",
              " 51,\n",
              " 52,\n",
              " 53,\n",
              " 54,\n",
              " 55,\n",
              " 56,\n",
              " 57,\n",
              " 58,\n",
              " 59,\n",
              " 60,\n",
              " 61,\n",
              " 62,\n",
              " 63,\n",
              " 64,\n",
              " 65,\n",
              " 66,\n",
              " 67,\n",
              " 68,\n",
              " 69,\n",
              " 70,\n",
              " 71,\n",
              " 72,\n",
              " 73,\n",
              " 74,\n",
              " 75,\n",
              " 76,\n",
              " 77,\n",
              " 78,\n",
              " 79,\n",
              " 80,\n",
              " 81,\n",
              " 82,\n",
              " 83,\n",
              " 84,\n",
              " 85,\n",
              " 86,\n",
              " 87,\n",
              " 88,\n",
              " 89,\n",
              " 90,\n",
              " 91,\n",
              " 92,\n",
              " 93,\n",
              " 94,\n",
              " 95,\n",
              " 96,\n",
              " 97,\n",
              " 98,\n",
              " 99]"
            ]
          },
          "metadata": {},
          "execution_count": 574
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Distribution of the treatments given in our data. (Most of the time no treatment is given, might vary on depending on how you cluster the data)"
      ],
      "metadata": {
        "id": "aKkw5PAlBeo9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_df.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AYFFwuT-hsP",
        "outputId": "54c86b04-746b-418a-c8bd-6183108dce67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0    195786\n",
              "1.0    135305\n",
              "3.0     15978\n",
              "2.0      8435\n",
              "Name: action, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 575
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Estimating the Transition Dynamics using the MLE (feel free to play around with the smoothing_value)"
      ],
      "metadata": {
        "id": "QOKcfQRcJPRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state_encoder = OneHotEncoder(sparse=False, categories= [np.arange(num_clusters)])\n",
        "action_encoder = OneHotEncoder(sparse=False, categories= [np.arange(action_bound)])\n",
        "\n",
        "states_onehot = state_encoder.fit_transform(X_df['cluster'].to_numpy().reshape(-1, 1))\n",
        "actions_onehot = action_encoder.fit_transform(y_df.to_numpy().reshape(-1, 1))"
      ],
      "metadata": {
        "id": "-IV4o95nRlJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "p_transition = np.zeros((num_clusters, num_clusters, action_bound)) + smoothing_value\n",
        "\n",
        "\n",
        "for traj in trajectories:\n",
        "\n",
        "  for tran in traj._t:\n",
        "\n",
        "    p_transition[tran[0], tran[2], tran[1]] +=1\n",
        "\n",
        "p_transition = p_transition/ p_transition.sum(axis = 1)[:, np.newaxis, :]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhzuEkc8RlFG",
        "outputId": "6cf49cb9-a930-4ad1-c0d7-f27609f26826"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[2.73511648e-01, 2.47170935e-01, 1.79856115e-01, 9.66386555e-02],\n",
              "        [8.62812770e-04, 5.95592615e-04, 3.59712230e-03, 4.20168067e-03],\n",
              "        [8.34052344e-03, 7.14711138e-03, 1.43884892e-02, 8.40336134e-03],\n",
              "        ...,\n",
              "        [2.87604257e-04, 5.95592615e-04, 3.59712230e-03, 4.20168067e-03],\n",
              "        [2.87604257e-04, 5.95592615e-04, 3.59712230e-03, 4.20168067e-03],\n",
              "        [5.75208513e-04, 1.78677784e-03, 3.59712230e-03, 4.20168067e-03]],\n",
              "\n",
              "       [[2.13675214e-03, 2.53807107e-03, 8.47457627e-03, 7.57575758e-03],\n",
              "        [1.56695157e-01, 8.88324873e-02, 8.47457627e-03, 1.51515152e-02],\n",
              "        [8.54700855e-03, 7.61421320e-03, 8.47457627e-03, 7.57575758e-03],\n",
              "        ...,\n",
              "        [1.42450142e-03, 2.53807107e-03, 8.47457627e-03, 7.57575758e-03],\n",
              "        [3.56125356e-03, 5.07614213e-03, 8.47457627e-03, 7.57575758e-03],\n",
              "        [6.41025641e-03, 1.52284264e-02, 8.47457627e-03, 7.57575758e-03]],\n",
              "\n",
              "       [[6.45855759e-03, 3.78266850e-03, 1.83150183e-02, 6.12244898e-03],\n",
              "        [2.15285253e-03, 6.87757909e-04, 3.66300366e-03, 2.04081633e-03],\n",
              "        [2.09257266e-01, 2.38995873e-01, 1.39194139e-01, 2.00000000e-01],\n",
              "        ...,\n",
              "        [6.45855759e-04, 3.43878955e-04, 3.66300366e-03, 2.04081633e-03],\n",
              "        [2.15285253e-04, 6.87757909e-04, 3.66300366e-03, 2.04081633e-03],\n",
              "        [2.15285253e-03, 6.87757909e-04, 3.66300366e-03, 2.04081633e-03]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[9.90099010e-04, 1.27388535e-03, 6.06060606e-03, 2.88184438e-03],\n",
              "        [3.96039604e-03, 6.36942675e-04, 6.06060606e-03, 2.88184438e-03],\n",
              "        [1.98019802e-03, 6.36942675e-04, 6.06060606e-03, 2.88184438e-03],\n",
              "        ...,\n",
              "        [2.78217822e-01, 4.02547771e-01, 1.09090909e-01, 2.85302594e-01],\n",
              "        [9.90099010e-03, 3.82165605e-03, 6.06060606e-03, 8.64553314e-03],\n",
              "        [9.90099010e-04, 1.27388535e-03, 6.06060606e-03, 5.76368876e-03]],\n",
              "\n",
              "       [[2.81690141e-03, 3.44827586e-03, 9.17431193e-03, 8.47457627e-03],\n",
              "        [8.45070423e-03, 6.89655172e-03, 9.17431193e-03, 8.47457627e-03],\n",
              "        [2.81690141e-03, 3.44827586e-03, 9.17431193e-03, 8.47457627e-03],\n",
              "        ...,\n",
              "        [3.09859155e-02, 1.37931034e-02, 1.83486239e-02, 1.69491525e-02],\n",
              "        [1.43661972e-01, 1.48275862e-01, 2.75229358e-02, 4.23728814e-02],\n",
              "        [2.81690141e-03, 6.89655172e-03, 9.17431193e-03, 8.47457627e-03]],\n",
              "\n",
              "       [[1.79425837e-03, 2.33463035e-03, 1.13636364e-02, 4.04858300e-03],\n",
              "        [4.78468900e-03, 3.11284047e-03, 5.68181818e-03, 4.04858300e-03],\n",
              "        [4.78468900e-03, 1.55642023e-03, 5.68181818e-03, 8.09716599e-03],\n",
              "        ...,\n",
              "        [5.98086124e-04, 7.78210117e-04, 5.68181818e-03, 4.04858300e-03],\n",
              "        [5.98086124e-04, 7.78210117e-04, 5.68181818e-03, 4.04858300e-03],\n",
              "        [2.06339713e-01, 2.46692607e-01, 1.02272727e-01, 1.33603239e-01]]])"
            ]
          },
          "metadata": {},
          "execution_count": 580
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Max Causal Entropy"
      ],
      "metadata": {
        "id": "n6WOgG24KKQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MaxEnt IRL"
      ],
      "metadata": {
        "id": "VFrongPAiw2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Maximum Entropy Inverse Reinforcement Learning and Maximum Causal Entropy\n",
        "Inverse Reinforcement Learning.\n",
        "\n",
        "Based on the corresponding paper by B. Ziebart et al. (2008) and the Thesis\n",
        "by Ziebart (2010).\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from itertools import product\n",
        "\n",
        "\n",
        "# -- common functions ----------------------------------------------------------\n",
        "\n",
        "def feature_expectation_from_trajectories(features, trajectories):\n",
        "    \"\"\"\n",
        "    Compute the feature expectation of the given trajectories.\n",
        "\n",
        "    Simply counts the number of visitations to each feature-instance and\n",
        "    divides them by the number of trajectories.\n",
        "\n",
        "    Args:\n",
        "        features: The feature-matrix (e.g. as numpy array), mapping states\n",
        "            to features, i.e. a matrix of shape (n_states x n_features).\n",
        "        trajectories: A list or iterator of `Trajectory` instances.\n",
        "\n",
        "    Returns:\n",
        "        The feature-expectation of the provided trajectories as map\n",
        "        `[state: Integer] -> feature_expectation: Float`.\n",
        "    \"\"\"\n",
        "    n_states, n_features = features.shape\n",
        "\n",
        "    fe = np.zeros(n_features)\n",
        "\n",
        "    for t in trajectories:\n",
        "        for s in t.states():\n",
        "            fe += features[s, :]\n",
        "\n",
        "    return fe / len(trajectories)\n",
        "\n",
        "\n",
        "def initial_probabilities_from_trajectories(n_states, trajectories):\n",
        "    \"\"\"\n",
        "    Compute the probability of a state being a starting state using the\n",
        "    given trajectories.\n",
        "\n",
        "    Args:\n",
        "        n_states: The number of states.\n",
        "        trajectories: A list or iterator of `Trajectory` instances.\n",
        "\n",
        "    Returns:\n",
        "        The probability of a state being a starting-state as map\n",
        "        `[state: Integer] -> probability: Float`.\n",
        "    \"\"\"\n",
        "    p = np.zeros(n_states)\n",
        "\n",
        "    for t in trajectories:\n",
        "        p[t.transitions()[0][0]] += 1.0\n",
        "\n",
        "    return p / len(trajectories)\n",
        "\n",
        "\n",
        "def expected_svf_from_policy(p_transition, p_initial, terminal, p_action, eps=1e-5):\n",
        "    \"\"\"\n",
        "    Compute the expected state visitation frequency using the given local\n",
        "    action probabilities.\n",
        "\n",
        "    This is the forward pass of Algorithm 1 of the Maximum Entropy IRL paper\n",
        "    by Ziebart et al. (2008). Alternatively, it can also be found as\n",
        "    Algorithm 9.3 in in Ziebart's thesis (2010).\n",
        "\n",
        "    It has been slightly adapted for convergence, by forcing transition\n",
        "    probabilities from terminal stats to be zero.\n",
        "\n",
        "    Args:\n",
        "        p_transition: The transition probabilities of the MDP as table\n",
        "            `[from: Integer, to: Integer, action: Integer] -> probability: Float`\n",
        "            specifying the probability of a transition from state `from` to\n",
        "            state `to` via action `action` to succeed.\n",
        "        p_initial: The probability of a state being an initial state as map\n",
        "            `[state: Integer] -> probability: Float`.\n",
        "        terminal: A list of terminal states.\n",
        "        p_action: Local action probabilities as map\n",
        "            `[state: Integer, action: Integer] -> probability: Float`\n",
        "            as returned by `local_action_probabilities`.\n",
        "        eps: The threshold to be used as convergence criterion. Convergence\n",
        "            is assumed if the expected state visitation frequency changes\n",
        "            less than the threshold on all states in a single iteration.\n",
        "\n",
        "    Returns:\n",
        "        The expected state visitation frequencies as map\n",
        "        `[state: Integer] -> svf: Float`.\n",
        "    \"\"\"\n",
        "    n_states, _, n_actions = p_transition.shape\n",
        "\n",
        "    # 'fix' our transition probabilities to allow for convergence\n",
        "    # we will _never_ leave any terminal state\n",
        "    p_transition = np.copy(p_transition)\n",
        "    p_transition[terminal, :, :] = 0.0\n",
        "\n",
        "    # set-up transition matrices for each action\n",
        "    p_transition = [np.array(p_transition[:, :, a]) for a in range(n_actions)]\n",
        "\n",
        "    # actual forward-computation of state expectations\n",
        "    d = np.zeros(n_states)\n",
        "\n",
        "    delta = np.inf\n",
        "    while delta > eps:\n",
        "        d_ = [p_transition[a].T.dot(p_action[:, a] * d) for a in range(n_actions)]\n",
        "        d_ = p_initial + np.array(d_).sum(axis=0)\n",
        "\n",
        "        delta, d = np.max(np.abs(d_ - d)), d_\n",
        "\n",
        "    return d\n",
        "\n",
        "\n",
        "# -- plain maximum entropy (Ziebart et al. 2008) -------------------------------\n",
        "\n",
        "def local_action_probabilities(p_transition, terminal, reward):\n",
        "    \"\"\"\n",
        "    Compute the local action probabilities (policy) required for the edge\n",
        "    frequency calculation for maximum entropy reinfocement learning.\n",
        "\n",
        "    This is the backward pass of Algorithm 1 of the Maximum Entropy IRL\n",
        "    paper by Ziebart et al. (2008).\n",
        "\n",
        "    Args:\n",
        "        p_transition: The transition probabilities of the MDP as table\n",
        "            `[from: Integer, to: Integer, action: Integer] -> probability: Float`\n",
        "            specifying the probability of a transition from state `from` to\n",
        "            state `to` via action `action` to succeed.\n",
        "        terminal: A set/list of terminal states.\n",
        "        reward: The reward signal per state as table\n",
        "            `[state: Integer] -> reward: Float`.\n",
        "\n",
        "    Returns:\n",
        "        The local action probabilities (policy) as map\n",
        "        `[state: Integer, action: Integer] -> probability: Float`\n",
        "    \"\"\"\n",
        "    n_states, _, n_actions = p_transition.shape\n",
        "\n",
        "    er = np.exp(reward)\n",
        "    p = [np.array(p_transition[:, :, a]) for a in range(n_actions)]\n",
        "\n",
        "    # initialize at terminal states\n",
        "    zs = np.zeros(n_states)\n",
        "    zs[terminal] = 1.0\n",
        "\n",
        "    # perform backward pass\n",
        "    # This does not converge, instead we iterate a fixed number of steps. The\n",
        "    # number of steps is chosen to reflect the maximum steps required to\n",
        "    # guarantee propagation from any state to any other state and back in an\n",
        "    # arbitrary MDP defined by p_transition.\n",
        "    for _ in range(2 * n_states):\n",
        "        za = np.array([er * p[a].dot(zs) for a in range(n_actions)]).T\n",
        "        zs = za.sum(axis=1)\n",
        "\n",
        "    # compute local action probabilities\n",
        "    return za / zs[:, None]\n",
        "\n",
        "\n",
        "def compute_expected_svf(p_transition, p_initial, terminal, reward, eps=1e-5):\n",
        "    \"\"\"\n",
        "    Compute the expected state visitation frequency for maximum entropy IRL.\n",
        "\n",
        "    This is an implementation of Algorithm 1 of the Maximum Entropy IRL\n",
        "    paper by Ziebart et al. (2008).\n",
        "\n",
        "    This function combines the backward pass implemented in\n",
        "    `local_action_probabilities` with the forward pass implemented in\n",
        "    `expected_svf_from_policy`.\n",
        "\n",
        "    Args:\n",
        "        p_transition: The transition probabilities of the MDP as table\n",
        "            `[from: Integer, to: Integer, action: Integer] -> probability: Float`\n",
        "            specifying the probability of a transition from state `from` to\n",
        "            state `to` via action `action` to succeed.\n",
        "        p_initial: The probability of a state being an initial state as map\n",
        "            `[state: Integer] -> probability: Float`.\n",
        "        terminal: A list of terminal states.\n",
        "        reward: The reward signal per state as table\n",
        "            `[state: Integer] -> reward: Float`.\n",
        "        eps: The threshold to be used as convergence criterion for the\n",
        "            expected state-visitation frequency. Convergence is assumed if\n",
        "            the expected state visitation frequency changes less than the\n",
        "            threshold on all states in a single iteration.\n",
        "\n",
        "    Returns:\n",
        "        The expected state visitation frequencies as map\n",
        "        `[state: Integer] -> svf: Float`.\n",
        "    \"\"\"\n",
        "    p_action = local_action_probabilities(p_transition, terminal, reward)\n",
        "    return expected_svf_from_policy(p_transition, p_initial, terminal, p_action, eps)\n",
        "\n",
        "\n",
        "def irl(p_transition, features, terminal, trajectories, optim, init, eps=1e-4, eps_esvf=1e-5):\n",
        "    \"\"\"\n",
        "    Compute the reward signal given the demonstration trajectories using the\n",
        "    maximum entropy inverse reinforcement learning algorithm proposed in the\n",
        "    corresponding paper by Ziebart et al. (2008).\n",
        "\n",
        "    Args:\n",
        "        p_transition: The transition probabilities of the MDP as table\n",
        "            `[from: Integer, to: Integer, action: Integer] -> probability: Float`\n",
        "            specifying the probability of a transition from state `from` to\n",
        "            state `to` via action `action` to succeed.\n",
        "        features: The feature-matrix (e.g. as numpy array), mapping states\n",
        "            to features, i.e. a matrix of shape (n_states x n_features).\n",
        "        terminal: A list of terminal states.\n",
        "        trajectories: A list of `Trajectory` instances representing the\n",
        "            expert demonstrations.\n",
        "        optim: The `Optimizer` instance to use for gradient-based\n",
        "            optimization.\n",
        "        init: The `Initializer` to use for initialization of the reward\n",
        "            function parameters.\n",
        "        eps: The threshold to be used as convergence criterion for the\n",
        "            reward parameters. Convergence is assumed if all changes in the\n",
        "            scalar parameters are less than the threshold in a single\n",
        "            iteration.\n",
        "        eps_svf: The threshold to be used as convergence criterion for the\n",
        "            expected state-visitation frequency. Convergence is assumed if\n",
        "            the expected state visitation frequency changes less than the\n",
        "            threshold on all states in a single iteration.\n",
        "\n",
        "    Returns:\n",
        "        The reward per state as table `[state: Integer] -> reward: Float`.\n",
        "    \"\"\"\n",
        "    n_states, _, n_actions = p_transition.shape\n",
        "    _, n_features = features.shape\n",
        "\n",
        "    # compute static properties from trajectories\n",
        "    e_features = feature_expectation_from_trajectories(features, trajectories)\n",
        "    p_initial = initial_probabilities_from_trajectories(n_states, trajectories)\n",
        "\n",
        "    # basic gradient descent\n",
        "    theta = init(n_features)\n",
        "    delta = np.inf\n",
        "\n",
        "    optim.reset(theta)\n",
        "    count = 0\n",
        "    while delta > eps:\n",
        "        if count > 10000:\n",
        "            print('diverges: max ent')\n",
        "            break\n",
        "        theta_old = theta.copy()\n",
        "\n",
        "        # compute per-state reward\n",
        "        reward = features.dot(theta)\n",
        "\n",
        "        # compute the gradient\n",
        "        e_svf = compute_expected_svf(p_transition, p_initial, terminal, reward, eps_esvf)\n",
        "        grad = e_features - features.T.dot(e_svf)\n",
        "\n",
        "        # perform optimization step and compute delta for convergence\n",
        "        optim.step(grad)\n",
        "        delta = np.max(np.abs(theta_old - theta))\n",
        "        count += 1\n",
        "\n",
        "    # re-compute per-state reward and return\n",
        "    return features.dot(theta)\n",
        "\n",
        "\n",
        "# -- maximum causal entropy (Ziebart 2010) -------------------------------------\n",
        "\n",
        "def softmax(x1, x2):\n",
        "    \"\"\"\n",
        "    Computes a soft maximum of both arguments.\n",
        "\n",
        "    In case `x1` and `x2` are arrays, computes the element-wise softmax.\n",
        "\n",
        "    Args:\n",
        "        x1: Scalar or ndarray.\n",
        "        x2: Scalar or ndarray.\n",
        "\n",
        "    Returns:\n",
        "        The soft maximum of the given arguments, either scalar or ndarray,\n",
        "        depending on the input.\n",
        "    \"\"\"\n",
        "    x_max = np.maximum(x1, x2)\n",
        "    x_min = np.minimum(x1, x2)\n",
        "    return x_max + np.log(1.0 + np.exp(x_min - x_max))\n",
        "\n",
        "\n",
        "def local_causal_action_probabilities(p_transition, terminal, reward, discount, eps=1e-5):\n",
        "    \"\"\"\n",
        "    Compute the local action probabilities (policy) required for the edge\n",
        "    frequency calculation for maximum causal entropy reinfocement learning.\n",
        "\n",
        "    This is Algorithm 9.1 from Ziebart's thesis (2010) combined with\n",
        "    discounting for convergence reasons as proposed in the same thesis.\n",
        "\n",
        "    Args:\n",
        "        p_transition: The transition probabilities of the MDP as table\n",
        "            `[from: Integer, to: Integer, action: Integer] -> probability: Float`\n",
        "            specifying the probability of a transition from state `from` to\n",
        "            state `to` via action `action` to succeed.\n",
        "        terminal: Either the terminal reward function or a collection of\n",
        "            terminal states. Iff `len(terminal)` is equal to the number of\n",
        "            states, it is assumed to contain the terminal reward function\n",
        "            (phi) as specified in Ziebart's thesis. Otherwise `terminal` is\n",
        "            assumed to be a collection of terminal states from which the\n",
        "            terminal reward function will be derived.\n",
        "        reward: The reward signal per state as table\n",
        "            `[state: Integer] -> reward: Float`.\n",
        "        discount: A discounting factor as Float.\n",
        "        eps: The threshold to be used as convergence criterion for the state\n",
        "            partition function. Convergence is assumed if the state\n",
        "            partition function changes less than the threshold on all states\n",
        "            in a single iteration.\n",
        "\n",
        "    Returns:\n",
        "        The local action probabilities (policy) as map\n",
        "        `[state: Integer, action: Integer] -> probability: Float`\n",
        "    \"\"\"\n",
        "    n_states, _, n_actions = p_transition.shape\n",
        "\n",
        "    # set up terminal reward function\n",
        "    if len(terminal) == n_states:\n",
        "        reward_terminal = np.array(terminal, dtype=float)\n",
        "    else:\n",
        "        reward_terminal = -np.inf * np.ones(n_states)\n",
        "        reward_terminal[terminal] = 0.0\n",
        "\n",
        "    # set up transition probability matrices\n",
        "    p = [np.array(p_transition[:, :, a]) for a in range(n_actions)]\n",
        "\n",
        "    # compute state log partition V and state-action log partition Q\n",
        "    v = -1e200 * np.ones(n_states)  # np.dot doesn't behave with -np.inf\n",
        "\n",
        "    delta = np.inf\n",
        "    while delta > eps:\n",
        "        v_old = v\n",
        "\n",
        "        q = np.array([reward + discount * p[a].dot(v_old) for a in range(n_actions)]).T\n",
        "\n",
        "        v = reward_terminal\n",
        "        for a in range(n_actions):\n",
        "            v = softmax(v, q[:, a])\n",
        "\n",
        "        # for some reason numpy chooses an array of objects after reduction, force floats here\n",
        "        v = np.array(v, dtype=float)\n",
        "\n",
        "        delta = np.max(np.abs(v - v_old))\n",
        "\n",
        "    # compute and return policy\n",
        "    return np.exp(q - v[:, None])\n",
        "\n",
        "\n",
        "def compute_expected_causal_svf(p_transition, p_initial, terminal, reward, discount,\n",
        "                                eps_lap=1e-5, eps_svf=1e-5):\n",
        "    \"\"\"\n",
        "    Compute the expected state visitation frequency for maximum causal\n",
        "    entropy IRL.\n",
        "\n",
        "    This is a combination of Algorithm 9.1 and 9.3 of Ziebart's thesis\n",
        "    (2010). See `local_causal_action_probabilities` and\n",
        "    `expected_svf_from_policy` for more details.\n",
        "\n",
        "    Args:\n",
        "        p_transition: The transition probabilities of the MDP as table\n",
        "            `[from: Integer, to: Integer, action: Integer] -> probability: Float`\n",
        "            specifying the probability of a transition from state `from` to\n",
        "            state `to` via action `action` to succeed.\n",
        "        p_initial: The probability of a state being an initial state as map\n",
        "            `[state: Integer] -> probability: Float`.\n",
        "        terminal: Either the terminal reward function or a collection of\n",
        "            terminal states. Iff `len(terminal)` is equal to the number of\n",
        "            states, it is assumed to contain the terminal reward function\n",
        "            (phi) as specified in Ziebart's thesis. Otherwise `terminal` is\n",
        "            assumed to be a collection of terminal states from which the\n",
        "            terminal reward function will be derived.\n",
        "        reward: The reward signal per state as table\n",
        "            `[state: Integer] -> reward: Float`.\n",
        "        discount: A discounting factor as Float.\n",
        "        eps_lap: The threshold to be used as convergence criterion for the\n",
        "            state partition function. Convergence is assumed if the state\n",
        "            partition function changes less than the threshold on all states\n",
        "            in a single iteration.\n",
        "        eps_svf: The threshold to be used as convergence criterion for the\n",
        "            expected state-visitation frequency. Convergence is assumed if\n",
        "            the expected state visitation frequency changes less than the\n",
        "            threshold on all states in a single iteration.\n",
        "    \"\"\"\n",
        "    p_action = local_causal_action_probabilities(p_transition, terminal, reward, discount, eps_lap)\n",
        "    return expected_svf_from_policy(p_transition, p_initial, terminal, p_action, eps_svf)\n",
        "\n",
        "\n",
        "def irl_causal(p_transition, features, terminal, trajectories, optim, init, discount,\n",
        "               eps=1e-4, eps_svf=1e-5, eps_lap=1e-5):\n",
        "    \"\"\"\n",
        "    Compute the reward signal given the demonstration trajectories using the\n",
        "    maximum causal entropy inverse reinforcement learning algorithm proposed\n",
        "    Ziebart's thesis (2010).\n",
        "\n",
        "    Args:\n",
        "        p_transition: The transition probabilities of the MDP as table\n",
        "            `[from: Integer, to: Integer, action: Integer] -> probability: Float`\n",
        "            specifying the probability of a transition from state `from` to\n",
        "            state `to` via action `action` to succeed.\n",
        "        features: The feature-matrix (e.g. as numpy array), mapping states\n",
        "            to features, i.e. a matrix of shape (n_states x n_features).\n",
        "        terminal: Either the terminal reward function or a collection of\n",
        "            terminal states. Iff `len(terminal)` is equal to the number of\n",
        "            states, it is assumed to contain the terminal reward function\n",
        "            (phi) as specified in Ziebart's thesis. Otherwise `terminal` is\n",
        "            assumed to be a collection of terminal states from which the\n",
        "            terminal reward function will be derived.\n",
        "        trajectories: A list of `Trajectory` instances representing the\n",
        "            expert demonstrations.\n",
        "        optim: The `Optimizer` instance to use for gradient-based\n",
        "            optimization.\n",
        "        init: The `Initializer` to use for initialization of the reward\n",
        "            function parameters.\n",
        "        discount: A discounting factor for the log partition functions as\n",
        "            Float.\n",
        "        eps: The threshold to be used as convergence criterion for the\n",
        "            reward parameters. Convergence is assumed if all changes in the\n",
        "            scalar parameters are less than the threshold in a single\n",
        "            iteration.\n",
        "        eps_lap: The threshold to be used as convergence criterion for the\n",
        "            state partition function. Convergence is assumed if the state\n",
        "            partition function changes less than the threshold on all states\n",
        "            in a single iteration.\n",
        "        eps_svf: The threshold to be used as convergence criterion for the\n",
        "            expected state-visitation frequency. Convergence is assumed if\n",
        "            the expected state visitation frequency changes less than the\n",
        "            threshold on all states in a single iteration.\n",
        "    \"\"\"\n",
        "    n_states, _, n_actions = p_transition.shape\n",
        "    _, n_features = features.shape\n",
        "\n",
        "    # compute static properties from trajectories\n",
        "    e_features = feature_expectation_from_trajectories(features, trajectories)\n",
        "    p_initial = initial_probabilities_from_trajectories(n_states, trajectories)\n",
        "\n",
        "    # basic gradient descent\n",
        "    theta = init(n_features)\n",
        "    delta = np.inf\n",
        "\n",
        "    optim.reset(theta)\n",
        "\n",
        "    count = 0\n",
        "    while delta > eps:\n",
        "        if count > 10000:\n",
        "            print('diverges: max causal')\n",
        "            break\n",
        "        theta_old = theta.copy()\n",
        "\n",
        "        # compute per-state reward\n",
        "        reward = features.dot(theta)\n",
        "\n",
        "        # compute the gradient\n",
        "        e_svf = compute_expected_causal_svf(p_transition, p_initial, terminal, reward, discount,\n",
        "                                            eps_lap, eps_svf)\n",
        "\n",
        "        grad = e_features - features.T.dot(e_svf)\n",
        "\n",
        "        # perform optimization step and compute delta for convergence\n",
        "        optim.step(grad)\n",
        "        delta = np.max(np.abs(theta_old - theta))\n",
        "\n",
        "        if count % 50 == 0:\n",
        "            print(delta)\n",
        "\n",
        "        count += 1\n",
        "\n",
        "    # re-compute per-state reward and return\n",
        "    return features.dot(theta)"
      ],
      "metadata": {
        "id": "NnWgDId_zNQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Max CAUSAL IRL"
      ],
      "metadata": {
        "id": "91838S-7z2Kd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Maximum Entropy Inverse Reinforcement Learning and Maximum Causal Entropy\n",
        "Inverse Reinforcement Learning.\n",
        "\n",
        "Based on the corresponding paper by B. Ziebart et al. (2008) and the Thesis\n",
        "by Ziebart (2010).\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from itertools import product\n",
        "\n",
        "\n",
        "# -- common functions ----------------------------------------------------------\n",
        "\n",
        "def feature_expectation_from_trajectories(features, trajectories):\n",
        "    \"\"\"\n",
        "    Compute the feature expectation of the given trajectories.\n",
        "\n",
        "    Simply counts the number of visitations to each feature-instance and\n",
        "    divides them by the number of trajectories.\n",
        "\n",
        "    Args:\n",
        "        features: The feature-matrix (e.g. as numpy array), mapping states\n",
        "            to features, i.e. a matrix of shape (n_states x n_features).\n",
        "        trajectories: A list or iterator of `Trajectory` instances.\n",
        "\n",
        "    Returns:\n",
        "        The feature-expectation of the provided trajectories as map\n",
        "        `[state: Integer] -> feature_expectation: Float`.\n",
        "    \"\"\"\n",
        "    n_states, n_features = features.shape\n",
        "\n",
        "    fe = np.zeros(n_features)\n",
        "\n",
        "    for t in trajectories:\n",
        "        for s in t.states():\n",
        "            fe += features[s, :]\n",
        "\n",
        "    return fe / len(trajectories)\n",
        "\n",
        "\n",
        "def initial_probabilities_from_trajectories(n_states, trajectories):\n",
        "    \"\"\"\n",
        "    Compute the probability of a state being a starting state using the\n",
        "    given trajectories.\n",
        "\n",
        "    Args:\n",
        "        n_states: The number of states.\n",
        "        trajectories: A list or iterator of `Trajectory` instances.\n",
        "\n",
        "    Returns:\n",
        "        The probability of a state being a starting-state as map\n",
        "        `[state: Integer] -> probability: Float`.\n",
        "    \"\"\"\n",
        "    p = np.zeros(n_states)\n",
        "\n",
        "    for t in trajectories:\n",
        "        p[t.transitions()[0][0]] += 1.0\n",
        "\n",
        "    return p / len(trajectories)\n",
        "\n",
        "\n",
        "def expected_svf_from_policy(p_transition, p_initial, terminal, p_action, eps=1e-5):\n",
        "    \"\"\"\n",
        "    Compute the expected state visitation frequency using the given local\n",
        "    action probabilities.\n",
        "\n",
        "    This is the forward pass of Algorithm 1 of the Maximum Entropy IRL paper\n",
        "    by Ziebart et al. (2008). Alternatively, it can also be found as\n",
        "    Algorithm 9.3 in in Ziebart's thesis (2010).\n",
        "\n",
        "    It has been slightly adapted for convergence, by forcing transition\n",
        "    probabilities from terminal stats to be zero.\n",
        "\n",
        "    Args:\n",
        "        p_transition: The transition probabilities of the MDP as table\n",
        "            `[from: Integer, to: Integer, action: Integer] -> probability: Float`\n",
        "            specifying the probability of a transition from state `from` to\n",
        "            state `to` via action `action` to succeed.\n",
        "        p_initial: The probability of a state being an initial state as map\n",
        "            `[state: Integer] -> probability: Float`.\n",
        "        terminal: A list of terminal states.\n",
        "        p_action: Local action probabilities as map\n",
        "            `[state: Integer, action: Integer] -> probability: Float`\n",
        "            as returned by `local_action_probabilities`.\n",
        "        eps: The threshold to be used as convergence criterion. Convergence\n",
        "            is assumed if the expected state visitation frequency changes\n",
        "            less than the threshold on all states in a single iteration.\n",
        "\n",
        "    Returns:\n",
        "        The expected state visitation frequencies as map\n",
        "        `[state: Integer] -> svf: Float`.\n",
        "    \"\"\"\n",
        "    n_states, _, n_actions = p_transition.shape\n",
        "\n",
        "    # 'fix' our transition probabilities to allow for convergence\n",
        "    # we will _never_ leave any terminal state\n",
        "    p_transition = np.copy(p_transition)\n",
        "    p_transition[terminal, :, :] = 0.0\n",
        "\n",
        "    # set-up transition matrices for each action\n",
        "    p_transition = [np.array(p_transition[:, :, a]) for a in range(n_actions)]\n",
        "\n",
        "    # actual forward-computation of state expectations\n",
        "    d = np.zeros(n_states)\n",
        "\n",
        "    delta = np.inf\n",
        "    while delta > eps:\n",
        "        d_ = [p_transition[a].T.dot(p_action[:, a] * d) for a in range(n_actions)]\n",
        "        d_ = p_initial + np.array(d_).sum(axis=0)\n",
        "\n",
        "        delta, d = np.max(np.abs(d_ - d)), d_\n",
        "\n",
        "    return d\n",
        "\n",
        "\n",
        "# -- plain maximum entropy (Ziebart et al. 2008) -------------------------------\n",
        "\n",
        "def local_action_probabilities(p_transition, terminal, reward):\n",
        "    \"\"\"\n",
        "    Compute the local action probabilities (policy) required for the edge\n",
        "    frequency calculation for maximum entropy reinfocement learning.\n",
        "\n",
        "    This is the backward pass of Algorithm 1 of the Maximum Entropy IRL\n",
        "    paper by Ziebart et al. (2008).\n",
        "\n",
        "    Args:\n",
        "        p_transition: The transition probabilities of the MDP as table\n",
        "            `[from: Integer, to: Integer, action: Integer] -> probability: Float`\n",
        "            specifying the probability of a transition from state `from` to\n",
        "            state `to` via action `action` to succeed.\n",
        "        terminal: A set/list of terminal states.\n",
        "        reward: The reward signal per state as table\n",
        "            `[state: Integer] -> reward: Float`.\n",
        "\n",
        "    Returns:\n",
        "        The local action probabilities (policy) as map\n",
        "        `[state: Integer, action: Integer] -> probability: Float`\n",
        "    \"\"\"\n",
        "    n_states, _, n_actions = p_transition.shape\n",
        "\n",
        "    er = np.exp(reward)\n",
        "    p = [np.array(p_transition[:, :, a]) for a in range(n_actions)]\n",
        "\n",
        "    # initialize at terminal states\n",
        "    zs = np.zeros(n_states)\n",
        "    zs[terminal] = 1.0\n",
        "\n",
        "    # perform backward pass\n",
        "    # This does not converge, instead we iterate a fixed number of steps. The\n",
        "    # number of steps is chosen to reflect the maximum steps required to\n",
        "    # guarantee propagation from any state to any other state and back in an\n",
        "    # arbitrary MDP defined by p_transition.\n",
        "    for _ in range(2 * n_states):\n",
        "        za = np.array([er * p[a].dot(zs) for a in range(n_actions)]).T\n",
        "        zs = za.sum(axis=1)\n",
        "\n",
        "    # compute local action probabilities\n",
        "    return za / zs[:, None]\n",
        "\n",
        "\n",
        "def compute_expected_svf(p_transition, p_initial, terminal, reward, eps=1e-5):\n",
        "    \"\"\"\n",
        "    Compute the expected state visitation frequency for maximum entropy IRL.\n",
        "\n",
        "    This is an implementation of Algorithm 1 of the Maximum Entropy IRL\n",
        "    paper by Ziebart et al. (2008).\n",
        "\n",
        "    This function combines the backward pass implemented in\n",
        "    `local_action_probabilities` with the forward pass implemented in\n",
        "    `expected_svf_from_policy`.\n",
        "\n",
        "    Args:\n",
        "        p_transition: The transition probabilities of the MDP as table\n",
        "            `[from: Integer, to: Integer, action: Integer] -> probability: Float`\n",
        "            specifying the probability of a transition from state `from` to\n",
        "            state `to` via action `action` to succeed.\n",
        "        p_initial: The probability of a state being an initial state as map\n",
        "            `[state: Integer] -> probability: Float`.\n",
        "        terminal: A list of terminal states.\n",
        "        reward: The reward signal per state as table\n",
        "            `[state: Integer] -> reward: Float`.\n",
        "        eps: The threshold to be used as convergence criterion for the\n",
        "            expected state-visitation frequency. Convergence is assumed if\n",
        "            the expected state visitation frequency changes less than the\n",
        "            threshold on all states in a single iteration.\n",
        "\n",
        "    Returns:\n",
        "        The expected state visitation frequencies as map\n",
        "        `[state: Integer] -> svf: Float`.\n",
        "    \"\"\"\n",
        "    p_action = local_action_probabilities(p_transition, terminal, reward)\n",
        "    return expected_svf_from_policy(p_transition, p_initial, terminal, p_action, eps)\n",
        "\n",
        "\n",
        "def irl(p_transition, features, terminal, trajectories, optim, init, eps=1e-4, eps_esvf=1e-5):\n",
        "    \"\"\"\n",
        "    Compute the reward signal given the demonstration trajectories using the\n",
        "    maximum entropy inverse reinforcement learning algorithm proposed in the\n",
        "    corresponding paper by Ziebart et al. (2008).\n",
        "\n",
        "    Args:\n",
        "        p_transition: The transition probabilities of the MDP as table\n",
        "            `[from: Integer, to: Integer, action: Integer] -> probability: Float`\n",
        "            specifying the probability of a transition from state `from` to\n",
        "            state `to` via action `action` to succeed.\n",
        "        features: The feature-matrix (e.g. as numpy array), mapping states\n",
        "            to features, i.e. a matrix of shape (n_states x n_features).\n",
        "        terminal: A list of terminal states.\n",
        "        trajectories: A list of `Trajectory` instances representing the\n",
        "            expert demonstrations.\n",
        "        optim: The `Optimizer` instance to use for gradient-based\n",
        "            optimization.\n",
        "        init: The `Initializer` to use for initialization of the reward\n",
        "            function parameters.\n",
        "        eps: The threshold to be used as convergence criterion for the\n",
        "            reward parameters. Convergence is assumed if all changes in the\n",
        "            scalar parameters are less than the threshold in a single\n",
        "            iteration.\n",
        "        eps_svf: The threshold to be used as convergence criterion for the\n",
        "            expected state-visitation frequency. Convergence is assumed if\n",
        "            the expected state visitation frequency changes less than the\n",
        "            threshold on all states in a single iteration.\n",
        "\n",
        "    Returns:\n",
        "        The reward per state as table `[state: Integer] -> reward: Float`.\n",
        "    \"\"\"\n",
        "    n_states, _, n_actions = p_transition.shape\n",
        "    _, n_features = features.shape\n",
        "\n",
        "    # compute static properties from trajectories\n",
        "    e_features = feature_expectation_from_trajectories(features, trajectories)\n",
        "    p_initial = initial_probabilities_from_trajectories(n_states, trajectories)\n",
        "\n",
        "    # basic gradient descent\n",
        "    theta = init(n_features)\n",
        "    delta = np.inf\n",
        "\n",
        "    optim.reset(theta)\n",
        "    while delta > eps:\n",
        "        theta_old = theta.copy()\n",
        "\n",
        "        # compute per-state reward\n",
        "        reward = features.dot(theta)\n",
        "\n",
        "        # compute the gradient\n",
        "        e_svf = compute_expected_svf(p_transition, p_initial, terminal, reward, eps_esvf)\n",
        "        grad = e_features - features.T.dot(e_svf)\n",
        "\n",
        "        # perform optimization step and compute delta for convergence\n",
        "        optim.step(grad)\n",
        "        delta = np.max(np.abs(theta_old - theta))\n",
        "\n",
        "    # re-compute per-state reward and return\n",
        "    return features.dot(theta)\n",
        "\n",
        "\n",
        "# -- maximum causal entropy (Ziebart 2010) -------------------------------------\n",
        "\n",
        "def softmax(x1, x2):\n",
        "    \"\"\"\n",
        "    Computes a soft maximum of both arguments.\n",
        "\n",
        "    In case `x1` and `x2` are arrays, computes the element-wise softmax.\n",
        "\n",
        "    Args:\n",
        "        x1: Scalar or ndarray.\n",
        "        x2: Scalar or ndarray.\n",
        "\n",
        "    Returns:\n",
        "        The soft maximum of the given arguments, either scalar or ndarray,\n",
        "        depending on the input.\n",
        "    \"\"\"\n",
        "    x_max = np.maximum(x1, x2)\n",
        "    x_min = np.minimum(x1, x2)\n",
        "    return x_max + np.log(1.0 + np.exp(x_min - x_max))\n",
        "\n",
        "\n",
        "def local_causal_action_probabilities(p_transition, terminal, reward, discount, eps=1e-5):\n",
        "    \"\"\"\n",
        "    Compute the local action probabilities (policy) required for the edge\n",
        "    frequency calculation for maximum causal entropy reinfocement learning.\n",
        "\n",
        "    This is Algorithm 9.1 from Ziebart's thesis (2010) combined with\n",
        "    discounting for convergence reasons as proposed in the same thesis.\n",
        "\n",
        "    Args:\n",
        "        p_transition: The transition probabilities of the MDP as table\n",
        "            `[from: Integer, to: Integer, action: Integer] -> probability: Float`\n",
        "            specifying the probability of a transition from state `from` to\n",
        "            state `to` via action `action` to succeed.\n",
        "        terminal: Either the terminal reward function or a collection of\n",
        "            terminal states. Iff `len(terminal)` is equal to the number of\n",
        "            states, it is assumed to contain the terminal reward function\n",
        "            (phi) as specified in Ziebart's thesis. Otherwise `terminal` is\n",
        "            assumed to be a collection of terminal states from which the\n",
        "            terminal reward function will be derived.\n",
        "        reward: The reward signal per state as table\n",
        "            `[state: Integer] -> reward: Float`.\n",
        "        discount: A discounting factor as Float.\n",
        "        eps: The threshold to be used as convergence criterion for the state\n",
        "            partition function. Convergence is assumed if the state\n",
        "            partition function changes less than the threshold on all states\n",
        "            in a single iteration.\n",
        "\n",
        "    Returns:\n",
        "        The local action probabilities (policy) as map\n",
        "        `[state: Integer, action: Integer] -> probability: Float`\n",
        "    \"\"\"\n",
        "    n_states, _, n_actions = p_transition.shape\n",
        "\n",
        "    # set up terminal reward function\n",
        "    if len(terminal) == n_states:\n",
        "        reward_terminal = np.array(terminal, dtype=float)\n",
        "    else:\n",
        "        reward_terminal = -np.inf * np.ones(n_states)\n",
        "        reward_terminal[terminal] = 0.0\n",
        "\n",
        "    # set up transition probability matrices\n",
        "    p = [np.array(p_transition[:, :, a]) for a in range(n_actions)]\n",
        "\n",
        "    # compute state log partition V and state-action log partition Q\n",
        "    v = -1e200 * np.ones(n_states)  # np.dot doesn't behave with -np.inf\n",
        "\n",
        "    delta = np.inf\n",
        "    while delta > eps:\n",
        "        v_old = v\n",
        "\n",
        "        q = np.array([reward + discount * p[a].dot(v_old) for a in range(n_actions)]).T\n",
        "\n",
        "        v = reward_terminal\n",
        "        for a in range(n_actions):\n",
        "            v = softmax(v, q[:, a])\n",
        "\n",
        "        # for some reason numpy chooses an array of objects after reduction, force floats here\n",
        "        v = np.array(v, dtype=float)\n",
        "\n",
        "        delta = np.max(np.abs(v - v_old))\n",
        "\n",
        "    # compute and return policy\n",
        "    return np.exp(q - v[:, None])\n",
        "\n",
        "\n",
        "def compute_expected_causal_svf(p_transition, p_initial, terminal, reward, discount,\n",
        "                                eps_lap=1e-5, eps_svf=1e-5):\n",
        "    \"\"\"\n",
        "    Compute the expected state visitation frequency for maximum causal\n",
        "    entropy IRL.\n",
        "\n",
        "    This is a combination of Algorithm 9.1 and 9.3 of Ziebart's thesis\n",
        "    (2010). See `local_causal_action_probabilities` and\n",
        "    `expected_svf_from_policy` for more details.\n",
        "\n",
        "    Args:\n",
        "        p_transition: The transition probabilities of the MDP as table\n",
        "            `[from: Integer, to: Integer, action: Integer] -> probability: Float`\n",
        "            specifying the probability of a transition from state `from` to\n",
        "            state `to` via action `action` to succeed.\n",
        "        p_initial: The probability of a state being an initial state as map\n",
        "            `[state: Integer] -> probability: Float`.\n",
        "        terminal: Either the terminal reward function or a collection of\n",
        "            terminal states. Iff `len(terminal)` is equal to the number of\n",
        "            states, it is assumed to contain the terminal reward function\n",
        "            (phi) as specified in Ziebart's thesis. Otherwise `terminal` is\n",
        "            assumed to be a collection of terminal states from which the\n",
        "            terminal reward function will be derived.\n",
        "        reward: The reward signal per state as table\n",
        "            `[state: Integer] -> reward: Float`.\n",
        "        discount: A discounting factor as Float.\n",
        "        eps_lap: The threshold to be used as convergence criterion for the\n",
        "            state partition function. Convergence is assumed if the state\n",
        "            partition function changes less than the threshold on all states\n",
        "            in a single iteration.\n",
        "        eps_svf: The threshold to be used as convergence criterion for the\n",
        "            expected state-visitation frequency. Convergence is assumed if\n",
        "            the expected state visitation frequency changes less than the\n",
        "            threshold on all states in a single iteration.\n",
        "    \"\"\"\n",
        "    p_action = local_causal_action_probabilities(p_transition, terminal, reward, discount, eps_lap)\n",
        "    return expected_svf_from_policy(p_transition, p_initial, terminal, p_action, eps_svf)\n",
        "\n",
        "\n",
        "def irl_causal(X_df, p_transition, features, terminal, trajectories, optim, init, discount,eps=1e-4, eps_svf=1e-5, eps_lap=1e-5):\n",
        "    \"\"\"\n",
        "    Compute the reward signal given the demonstration trajectories using the\n",
        "    maximum causal entropy inverse reinforcement learning algorithm proposed\n",
        "    Ziebart's thesis (2010).\n",
        "\n",
        "    Args:\n",
        "        p_transition: The transition probabilities of the MDP as table\n",
        "            `[from: Integer, to: Integer, action: Integer] -> probability: Float`\n",
        "            specifying the probability of a transition from state `from` to\n",
        "            state `to` via action `action` to succeed.\n",
        "        features: The feature-matrix (e.g. as numpy array), mapping states\n",
        "            to features, i.e. a matrix of shape (n_states x n_features).\n",
        "        terminal: Either the terminal reward function or a collection of\n",
        "            terminal states. Iff `len(terminal)` is equal to the number of\n",
        "            states, it is assumed to contain the terminal reward function\n",
        "            (phi) as specified in Ziebart's thesis. Otherwise `terminal` is\n",
        "            assumed to be a collection of terminal states from which the\n",
        "            terminal reward function will be derived.\n",
        "        trajectories: A list of `Trajectory` instances representing the\n",
        "            expert demonstrations.\n",
        "        optim: The `Optimizer` instance to use for gradient-based\n",
        "            optimization.\n",
        "        init: The `Initializer` to use for initialization of the reward\n",
        "            function parameters.\n",
        "        discount: A discounting factor for the log partition functions as\n",
        "            Float.\n",
        "        eps: The threshold to be used as convergence criterion for the\n",
        "            reward parameters. Convergence is assumed if all changes in the\n",
        "            scalar parameters are less than the threshold in a single\n",
        "            iteration.\n",
        "        eps_lap: The threshold to be used as convergence criterion for the\n",
        "            state partition function. Convergence is assumed if the state\n",
        "            partition function changes less than the threshold on all states\n",
        "            in a single iteration.\n",
        "        eps_svf: The threshold to be used as convergence criterion for the\n",
        "            expected state-visitation frequency. Convergence is assumed if\n",
        "            the expected state visitation frequency changes less than the\n",
        "            threshold on all states in a single iteration.\n",
        "    \"\"\"\n",
        "    n_states, _, n_actions = p_transition.shape\n",
        "    _, n_features = features.shape\n",
        "\n",
        "    # compute static properties from trajectories\n",
        "    e_features = feature_expectation_from_trajectories(features, trajectories)\n",
        "    p_initial = initial_probabilities_from_trajectories(n_states, trajectories)\n",
        "\n",
        "    # basic gradient descent\n",
        "    theta = init(n_features)\n",
        "    delta = np.inf\n",
        "\n",
        "    # input data\n",
        "    epochs_infos = []\n",
        "\n",
        "    optim.reset(theta)\n",
        "    count = 0\n",
        "    while delta > eps:\n",
        "        if count > 10000:\n",
        "            print(\"does not converge\")\n",
        "            break\n",
        "\n",
        "        theta_old = theta.copy()\n",
        "\n",
        "        # compute per-state reward\n",
        "        reward = features.dot(theta)\n",
        "\n",
        "        epochs_infos.append(reward.tolist())\n",
        "\n",
        "        # compute the gradient\n",
        "        e_svf = compute_expected_causal_svf(p_transition, p_initial, terminal, reward, discount,\n",
        "                                            eps_lap, eps_svf)\n",
        "\n",
        "        grad = e_features - features.T.dot(e_svf)\n",
        "\n",
        "        # perform optimization step and compute delta for convergence\n",
        "        optim.step(grad)\n",
        "        delta = np.max(np.abs(theta_old - theta))\n",
        "        count +=1\n",
        "\n",
        "    # re-compute per-state reward and return\n",
        "    final_reward =features.dot(theta)\n",
        "    epochs_infos.append(final_reward.tolist())\n",
        "\n",
        "    return epochs_infos, final_reward\n",
        "\n",
        "def update(reward_list, X_df):\n",
        "\n",
        "    rankings = list(np.argsort(reward_list))\n",
        "    # lowest to highest\n",
        "    all_cluster_info = {}\n",
        "    for i in range(len(reward_list)):\n",
        "        # reward_maxent_causal[i] is the reward value\n",
        "        cluster_id = i+1\n",
        "        reward_value = reward_list[i]\n",
        "        data = X_df[(X_df['cluster'] == i+1)]\n",
        "        all_cluster_info[cluster_id] = data.mean().to_dict()\n",
        "        all_cluster_info[cluster_id]['reward_value'] = reward_value\n",
        "        # note: rankings are 0-indexed with the unhealthiest first (lowest)\n",
        "        all_cluster_info[cluster_id]['ranking'] = rankings.index(i)\n",
        "    # add to history\n",
        "    return all_cluster_info\n",
        "\n",
        "def extract_values(discount_val, features, cluster_algo, smooth_val, learning_rate=0.2, data=''):\n",
        "  # choose our parameter initialization strategy:\n",
        "  #   initialize parameters with constant\n",
        "  init = O.Constant(1.0)\n",
        "\n",
        "  # choose our optimization strategy:\n",
        "  #   we select exponentiated stochastic gradient descent with linear learning-rate decay\n",
        "  optim = O.ExpSga(lr=O.linear_decay(lr0=learning_rate))\n",
        "\n",
        "  # actually do some inverse reinforcement learning\n",
        "  # reward_maxent = maxent_irl(p_transition, features, terminal_states, trajectories, optim, init, eps= 1e-3)\n",
        "\n",
        "  # send in the X_df dataframe\n",
        "  epochs_infos, final_reward = irl_causal(data_non_normalized_df, p_transition, features, terminal_states, trajectories, optim, init, discount_val, eps=1e-3, eps_svf=1e-4, eps_lap=1e-4)\n",
        "\n",
        "  if data != '':\n",
        "    file_name = data+'_mce_' + cluster_algo + str(discount_val)+'_smooth'+str(smooth_val)+'_lr'+str(learning_rate)+'.json'\n",
        "  else:\n",
        "    file_name = '_mce_' + cluster_algo + str(discount_val)+'_smooth'+str(smooth_val)+'.json'\n",
        "\n",
        "  final_cluster_info = update(final_reward, data_non_normalized_df)\n",
        "  print(\"----\")\n",
        "  print(final_cluster_info)\n",
        "  print(\"----\")\n",
        "  # print(epochs_infos)\n",
        "  print(\"---\")\n",
        "  print(np.array(epochs_infos).shape)\n",
        "  with open(file_name, 'w') as m:\n",
        "    json.dump(epochs_infos, m)\n",
        "\n",
        "  return epochs_infos"
      ],
      "metadata": {
        "id": "1sfW1zW3GGX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_reward_values = extract_values(discount_exp, features, 'kmeans', smoothing_val_exp, learning_rate=0.1, data='pneumonia')\n"
      ],
      "metadata": {
        "id": "VdHIJly2n_DD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}